{
  
    
        "post0": {
            "title": "Logistic model quasi-Laplace with Mr.Ash",
            "content": "About . This is a proof of concept implementation of quasi-Laplace approximation with Mr.Ash. The quasi-Laplace approximation introduces a regularizer on the likelihood and approximates the posterior with a Gaussian distribution. This approach can be used for generalized linear model with any link function. . Bayesian logistic regression with a Gaussian prior (ridge regression) uses the Laplace approximation to approximate the posterior with a Gaussian distribution. The variational treatment of logistic model by Jordan and Jaakkola (2000) also leads to a Gaussian approximation of the posterior distribution. The greater flexibility of the variational approximation leads to improved accuracy compared to Laplace method. . Here, I compare the quasi-Laplace approach with cross-validated Lasso with $L_1$ penalty. I used the $R^2$ statistic of Tjur 1 to compare the quality of class prediction, and the Gini index 2 for sparsity. . 1. Tjur, T. (2009). Coefficients of Determination in Logistic Regression Models - A New Proposal: The Coefficient of Discrimination. The American Statistician, 63(4), 366-372. DOI: 10.1198/tast.2009.08210↩ . 2. For a review of sparsity indices, see Harley and Rickard (2009). Comparing Measures of Sparsity. arXiv. DOI: arXiv:0811.4706v2↩ . . Warning: Coefficients are underestimated by QL irrespective of the choice of regularizer. Lasso overestimates the coefficients. . Note: Slight improvements in coefficient determination with optimal Gaussian mixture family. . #collapse_hide import numpy as np np.set_printoptions(precision = 4, suppress=True) import pandas as pd from scipy import optimize from scipy import stats from sklearn import linear_model import matplotlib.pyplot as plt from matplotlib import cm from matplotlib import ticker as plticker from mpl_toolkits.axes_grid1 import make_axes_locatable import sys sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . Generate toy data . Let us consider a logistic model with sparse coefficients, so that the number of causal variables ncausal is much less than the number of variables nvar in the model. This is ensured by sampling the betas from a Gaussian mixture prior of nGcomp components with variances given by $ sigma_k^2$ (sigmak2) and unknown mixture coefficients. The sparsity of the initialization is controlled by the variable sparsity that specifies the mixture coefficient of the component $ mathcal{N}(0, 0)$ (or the delta function). We are particularly interested for systems where the number of samples nsample is less than nvar. Validation is performed on separate test dataset with ntest samples. . nsample = [50, 200] nvar = [100, 100] ncausal = 3 nGcomp = 10 sparsity = 0.95 prior_variance = 5 ntest = 2000 hg2 = 0.8 sigmak2 = np.array([prior_variance * np.square(np.power(2, (i)/nGcomp) - 1) for i in range(nGcomp)]) . The data is generated from a liability threshold model such that the explanatory variables explain only a given fraction hg2 of the response variable. $ mathbf{X}$ is centered and scaled such that for each variable $j$, the variance is $ mathrm{var}( mathbf{x}_j) = 1$. . # collapse-hide def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) Xstd = Xnorm / np.std(Xnorm, axis = 0) return Xstd def logistic_data(X, beta): Xbeta = np.dot(X, beta) pred = 1 / (1 + np.exp(-Xbeta)) Y = np.random.binomial(1, pred) return Y def liability_model(X, beta, prevalence = 0.5): hg2 = np.sum(np.square(beta)) eff = np.dot(X, beta) rand_std = np.sqrt(1 - hg2) rand_eff = np.random.normal(0, rand_std, X.shape[0]) liability = eff + rand_eff cutoff = stats.norm.ppf(1 - prevalence) cases = np.where(liability &gt;= cutoff)[0] ctrls = np.where(liability &lt; cutoff)[0] Y = np.zeros(X.shape[0]) Y[cases] = 1 return Y, liability def tjur_R2(X, Y, beta): Xbeta = np.dot(X, beta) pred = 1 / (1 + np.exp(-Xbeta)) R2 = np.mean(pred[Y == 1]) - np.mean(pred[Y == 0]) return R2 def gini(x): n = len(x) xabs = abs(x) xsort = xabs[np.argsort(xabs)] t1 = xsort / np.sum(xabs) t2 = (n - np.arange(n) - 1 + 0.5) / n gini = 1 - 2 * np.sum(t1 * t2) return gini def simulate_data(nsample, nvar, ncausal, ntest, hg2): X = np.random.rand(nsample * nvar).reshape(nsample, nvar) X = standardize(X) Xtest = np.random.rand(ntest * nvar).reshape(ntest, nvar) Xtest = standardize(Xtest) beta = np.zeros(nvar) causal_idx = np.random.choice(nvar, ncausal, replace = False) beta[causal_idx] = np.random.normal(loc = 0., scale = 1., size = ncausal) beta = beta * np.sqrt(hg2 / np.sum(np.square(beta))) Y, Y_liab = liability_model(X, beta) Ytest, Ytest_liab = liability_model(Xtest, beta) return X, Y, beta, Xtest, Ytest, Y_liab, Ytest_liab . . Let us have a look at the generated data. . # collapse-hide X = [None for n in nsample] Xtest = [None for n in nsample] Y = [None for n in nsample] Ytest = [None for n in nsample] Y_liab = [None for n in nsample] Ytest_liab = [None for n in nsample] beta = [None for n in nsample] for i, n in enumerate(nsample): X[i], Y[i], beta[i], Xtest[i], Ytest[i], Y_liab[i], Ytest_liab[i] = simulate_data(n, nvar[i], ncausal, ntest, hg2) print(f&#39;There are {ncausal} non-zero coefficients with heritability {hg2:.4f}&#39;) fig = plt.figure(figsize = (12,6)) ax = [fig.add_subplot(121), fig.add_subplot(122)] for i, n in enumerate(nsample): Xbeta = np.dot(X[i], beta[i]) pred = 1 / (1 + np.exp(-Xbeta)) ax[i].scatter(Xbeta, Y[i], s = 10) ax[i].scatter(Xbeta, pred, s = 10) ax[i].set_xlabel(r&#39;$ sum_i X_{ni} beta_i$&#39;) ax[i].set_ylabel(r&#39;$Y_n$&#39;) plt.tight_layout() plt.show() . . There are 3 non-zero coefficients with heritability 0.8000 . Implementation of quasi-Laplace with variational approximation . See Latex notes for details of the theory. Current implementation focus on readability and debug. Computational efficiency has been ignored. . Warning: In some edge cases, the ELBO decreases with parameter update. Must be an error. Debug! . # collapse-hide def regularized_log_likelihood(beta, X, Y, L): nvar = beta.shape[0] Xbeta = np.dot(X, beta) ## Function llb = np.sum(np.dot(Y, Xbeta) - np.log(1 + np.exp(Xbeta))) reg = - 0.5 * np.einsum(&#39;i,i-&gt;i&#39;, np.square(beta), L) loglik = llb + reg ## Gradient pred = 1 / (1 + np.exp(-Xbeta)) der = np.einsum(&#39;i,ij-&gt;j&#39;, (Y - pred), X) - np.multiply(beta, L) return -loglik, -der def precisionLL(X, beta, L): nvar = X.shape[1] Xbeta = np.einsum(&#39;i,ji-&gt;j&#39;, beta, X) pred = 1 / (1 + np.exp(-Xbeta)) Sinv = np.einsum(&#39;i,i,ij,ik-&gt;jk&#39;, pred, (1 - pred), X, X) Sinv[np.diag_indices(nvar)] += L return Sinv def get_regularized_mode(X, Y, beta0, L): nsample, nvar = X.shape args = X, Y, L gmode = optimize.minimize(regularized_log_likelihood, beta0, args=args, method=&#39;L-BFGS-B&#39;, jac=True, bounds=None, options={&#39;maxiter&#39;: 20000000, &#39;maxfun&#39;: 20000000, &#39;ftol&#39;: 1e-9, &#39;gtol&#39;: 1e-9 #&#39;disp&#39;: True }) return gmode.x def get_elbo(alpha, mu, s2, pi, sigmak2, XPX, SinvM, M, L, logdetS): nvar, nGcomp = alpha.shape Ebeta = np.sum(np.multiply(alpha, mu)[:, 1:], axis = 1) mu2 = np.square(mu) logdetL = np.sum(np.log(L)) t0 = - 0.5 * (logdetL + logdetS + np.dot(M.T, SinvM)) t1 = - 0.5 * np.dot(Ebeta.T, np.dot(XPX, Ebeta)) t2 = np.dot(Ebeta.T, SinvM) t3 = 0 t4 = 0 for j in range(nvar): Ebeta2j = 0 for k in range(nGcomp): mom2jk = mu2[j, k] + s2[j, k] Ebeta2j += alpha[j, k] * mom2jk t4jk = 0 if alpha[j, k] != 0: t4jk += np.log(pi[k]) t4jk += - np.log(alpha[j, k]) if k &gt; 0: t4jk += 0.5 * np.log(s2[j, k]) t4jk += - 0.5 * np.log(sigmak2[k]) t4jk += - 0.5 * mom2jk / sigmak2[k] t4jk += 0.5 t4jk *= alpha[j, k] t4 += t4jk t3 += 0.5 * XPX[j, j] * (Ebeta[j] * Ebeta[j] - Ebeta2j) elbo = t0 + t1 + t2 + t3 + t4 return elbo def ash_qL_regularizer(X, Y, binit, L, mode_optim = True): nsample, nvar = X.shape if mode_optim: M = get_regularized_mode(X, Y, binit, L) else: M = binit.copy() Sinv = precisionLL(X, M, L) S = np.linalg.inv(Sinv) sgndetS, logdetS = np.linalg.slogdet(S) XPX = Sinv.copy() XPX[np.diag_indices(nvar)] -= L return M, S, Sinv, logdetS, XPX def ash_qL_pi_init(nGcomp, pi0, sparse = True): &#39;&#39;&#39; By default, pi is initialized with sparsity, i.e. pi[0] &gt; pi[j], for j &gt; 0 &#39;&#39;&#39; pi = np.zeros(nGcomp) pi[0] = pi0 pi[1:(nGcomp - 1)] = (1 - pi0) / (nGcomp - 1) pi[nGcomp - 1] = 1 - np.sum(pi) if not sparse: pi = np.repeat(1 / nGcomp, nGcomp) return pi def ash_logistic_qL(X, Y, nGcomp, sigmak2, binit, Linit, a_dirich = None, maxiter = 1000, tol = 1e-10, pi0 = 0.8, sparse_init = True, reg_step = 1, reg_tol = 0.1, debug = True, debug_step = 10): &#39;&#39;&#39; Current focus is on readability and debug. &#39;&#39;&#39; nsample, nvar = X.shape # default dirichlet prior on pi if a_dirich is None: a_dirich = np.repeat(1., nGcomp) a_dirich[0] = 5. # soft max at pi[0] &#39;&#39;&#39; Initialize &#39;&#39;&#39; L = Linit.copy() M, S, Sinv, logdetS, XPX = ash_qL_regularizer(X, Y, binit, L) SinvM = np.dot(Sinv, M) SinvRtj = np.dot(Sinv, M) - np.multiply(np.diag(Sinv), M) pi = ash_qL_pi_init(nGcomp, pi0, sparse = sparse_init) alpha = np.repeat(np.array([pi]), nvar, axis = 0) logalpha = np.log(alpha) s2 = np.zeros((nvar, nGcomp)) s2[:, 1:] = 1 / (np.diag(XPX).reshape(-1, 1) + 1 / sigmak2[1:]) mu = np.multiply(s2, (SinvM - SinvRtj).reshape(-1,1)) R = np.sum(np.multiply(alpha, mu)[:, 1:], axis = 1) &#39;&#39;&#39; Iteration &#39;&#39;&#39; elbo_old = get_elbo(alpha, mu, s2, pi, sigmak2, XPX, SinvM, M, L, logdetS) pi_old = pi.copy() pi_reg = pi.copy() if debug: print(f&quot;Starting ELBO: {elbo_old:.4f} Pi0: {pi[0]:.4f}&quot;) for it in range(maxiter): # E-step for j in range(nvar): SinvRtj = np.sum(np.multiply(Sinv[:, j], R[:])) - Sinv[j, j] * R[j] s2[j, 0] = 0. mu[j, 0] = 0. logalpha[j, 0] = np.log(pi[0]) for k in range(1, nGcomp): s2[j, k] = 1 / (XPX[j, j] + (1 / sigmak2[k])) mu[j, k] = s2[j, k] * (SinvM[j] - SinvRtj) ssf = np.log(s2[j, k] / sigmak2[k]) ssr = mu[j, k] * mu[j, k] / s2[j, k] logalpha[j, k] = np.log(pi[k]) + 0.5 * ssf + 0.5 * ssr maxL = np.max(logalpha[j, :]) alpha[j, :] = np.exp(logalpha[j, :] - maxL) alpha[j, :] /= np.sum(alpha[j, :]) R[j] = np.sum(np.multiply(alpha[j, 1:], mu[j, 1:])) # M-step bk = np.sum(alpha, axis = 0) + a_dirich pi = bk / np.sum(bk) #pi = np.exp(digamma(bk) - digamma(np.sum(bk))) #pi /= np.sum(pi) #for k in range(nGcomp): # pi[k] = np.sum(alpha[:, k]) / nvar # Conditional M-step if (it + 1) % reg_step == 0: eps_reg = max([abs(x - y) for x, y in zip(pi, pi_reg)]) if eps_reg &gt; reg_tol: print(f&quot;Recalculating regularizer, step {it}&quot;) L = 1 / np.sum(alpha * sigmak2, axis = 1) M, S, Sinv, logdetS, XPX = ash_qL_regularizer(X, Y, R, L) SinvM = np.dot(Sinv, M) pi_reg = pi.copy() elbo = get_elbo(alpha, mu, s2, pi, sigmak2, XPX, SinvM, M, L, logdetS) eps_elbo = elbo - elbo_old eps_pi = max([abs(x - y) for x, y in zip(pi, pi_old)]) if debug and (it % debug_step == 0): print(f&quot;Iteration {it}. ELBO: {elbo:.4f} Pi0: {pi[0]:.4f} epsilon: {eps_pi:.4f}, {eps_elbo:.4f}&quot;) &#39;&#39;&#39; Check termination, continue otherwise. &#39;&#39;&#39; if eps_pi &lt; tol: print(eps_pi) break &#39;&#39;&#39; Keep this step in memory. Use deep copy to avoid in place substitutions. &#39;&#39;&#39; pi_old = pi.copy() alpha_old = alpha.copy() mu_old = mu.copy() s2_old = s2.copy() R_old = R.copy() elbo_old = elbo return pi, alpha, mu, s2, R, L . . Hold the result summary statistic in a data frame . # collapse- hide res_df = [pd.DataFrame(columns = [&#39;Method&#39;, &#39;RMSE&#39;, &#39;Tjur R2&#39;, &#39;Gini Index&#39;]) for n in nsample] data_Xbeta = [np.dot(Xtest[i], beta[i]) for i in range(len(nsample))] data_tjur = [tjur_R2(Xtest[i], Ytest[i], beta[i]) for i in range(len(nsample))] data_rmse = [np.sqrt(np.mean(np.square(data_Xbeta[i] - np.dot(Xtest[i], beta[i])))) for i in range(len(nsample))] data_gini = [gini(beta[i]) for i in range(len(nsample))] for i in range(len(nsample)): res_df[i] = res_df[i].append(pd.DataFrame([[&#39;Data&#39;, data_rmse[i], data_tjur[i], data_gini[i]]], columns = res_df[i].columns)) . Fitting . Lasso | # collapse-hide clf = [linear_model.LogisticRegressionCV(cv = 5, penalty=&#39;l1&#39;, solver=&#39;saga&#39;, fit_intercept = False, max_iter = 5000) for n in nsample] for i, n in enumerate(nsample): clf[i].fit(X[i], Y[i]) clf_Xbeta = [np.dot(Xtest[i], clf[i].coef_[0]) for i in range(len(nsample))] clf_tjur = [tjur_R2(Xtest[i], Ytest[i], clf[i].coef_[0]) for i in range(len(nsample))] clf_rmse = [np.sqrt(np.mean(np.square(clf_Xbeta[i] - np.dot(Xtest[i], beta[i])))) for i in range(len(nsample))] clf_gini = [gini(clf[i].coef_[0]) for i in range(len(nsample))] for i in range(len(nsample)): res_df[i] = res_df[i].append(pd.DataFrame([[&#39;LASSO&#39;, clf_rmse[i], clf_tjur[i], clf_gini[i]]], columns = res_df[i].columns)) . . QL Ash | # collapse-hide pi = [None for n in nsample] alpha = [None for n in nsample] mu = [None for n in nsample] s2 = [None for n in nsample] R = [None for n in nsample] L = [None for n in nsample] for i in range(len(nsample)): gold_reg = np.repeat(1e4, nvar[i]) for j, b in enumerate(beta[i]): if b != 0: gold_reg[j] = 1 / prior_variance fix_reg = np.repeat(1 / sigmak2[nGcomp - 1], nvar[i]) binit = np.zeros(nvar[i]) Linit = fix_reg pi[i], alpha[i], mu[i], s2[i], R[i], L[i] = ash_logistic_qL(X[i], Y[i], nGcomp, sigmak2, binit, Linit, tol = 1e-3, debug_step = 1, maxiter = 1000, reg_step = 10, pi0 = sparsity, sparse_init = True) qLash_Xbeta = [np.dot(Xtest[i], R[i]) for i in range(len(nsample))] qLash_rmse = [np.sqrt(np.mean(np.square(qLash_Xbeta[i] - np.dot(Xtest[i], beta[i])))) for i in range(len(nsample))] qLash_r2 = [tjur_R2(Xtest[i], Ytest[i], R[i]) for i in range(len(nsample))] qLash_gini = [gini(R[i]) for i in range(len(nsample))] for i in range(len(nsample)): res_df[i] = res_df[i].append(pd.DataFrame([[&#39;qLash&#39;, qLash_rmse[i], qLash_r2[i], qLash_gini[i]]], columns = res_df[i].columns)) . . Starting ELBO: 28.4164 Pi0: 0.9500 Iteration 0. ELBO: 25.3210 Pi0: 0.8843 epsilon: 0.0657, -3.0954 Iteration 1. ELBO: 25.9625 Pi0: 0.8346 epsilon: 0.0497, 0.6415 Iteration 2. ELBO: 25.8847 Pi0: 0.7957 epsilon: 0.0389, -0.0778 Iteration 3. ELBO: 25.7156 Pi0: 0.7644 epsilon: 0.0313, -0.1691 Iteration 4. ELBO: 25.5541 Pi0: 0.7386 epsilon: 0.0257, -0.1615 Iteration 5. ELBO: 25.4176 Pi0: 0.7171 epsilon: 0.0216, -0.1365 Iteration 6. ELBO: 25.3058 Pi0: 0.6988 epsilon: 0.0183, -0.1118 Iteration 7. ELBO: 25.2148 Pi0: 0.6830 epsilon: 0.0157, -0.0910 Iteration 8. ELBO: 25.1404 Pi0: 0.6694 epsilon: 0.0136, -0.0744 Recalculating regularizer, step 9 Iteration 9. ELBO: -11.2161 Pi0: 0.6575 epsilon: 0.0119, -36.3565 Iteration 10. ELBO: 0.3987 Pi0: 0.6786 epsilon: 0.0211, 11.6148 Iteration 11. ELBO: 1.3344 Pi0: 0.6899 epsilon: 0.0113, 0.9357 Iteration 12. ELBO: 1.7018 Pi0: 0.6961 epsilon: 0.0061, 0.3674 Iteration 13. ELBO: 1.8795 Pi0: 0.6992 epsilon: 0.0031, 0.1778 Iteration 14. ELBO: 1.9770 Pi0: 0.7007 epsilon: 0.0022, 0.0974 Iteration 15. ELBO: 2.0341 Pi0: 0.7013 epsilon: 0.0019, 0.0572 Iteration 16. ELBO: 2.0692 Pi0: 0.7014 epsilon: 0.0017, 0.0351 Iteration 17. ELBO: 2.0914 Pi0: 0.7011 epsilon: 0.0015, 0.0222 Iteration 18. ELBO: 2.1057 Pi0: 0.7007 epsilon: 0.0013, 0.0143 Iteration 19. ELBO: 2.1151 Pi0: 0.7002 epsilon: 0.0012, 0.0094 Iteration 20. ELBO: 2.1213 Pi0: 0.6996 epsilon: 0.0011, 0.0062 Iteration 21. ELBO: 2.1253 Pi0: 0.6990 epsilon: 0.0010, 0.0040 0.0009842216645820079 Starting ELBO: 79.7171 Pi0: 0.9500 Iteration 0. ELBO: 83.2410 Pi0: 0.8977 epsilon: 0.0523, 3.5239 Iteration 1. ELBO: 83.0654 Pi0: 0.8711 epsilon: 0.0266, -0.1756 Iteration 2. ELBO: 82.7727 Pi0: 0.8543 epsilon: 0.0169, -0.2927 Iteration 3. ELBO: 82.5729 Pi0: 0.8423 epsilon: 0.0120, -0.1998 Iteration 4. ELBO: 82.4360 Pi0: 0.8331 epsilon: 0.0091, -0.1369 Iteration 5. ELBO: 82.3380 Pi0: 0.8259 epsilon: 0.0073, -0.0981 Iteration 6. ELBO: 82.2649 Pi0: 0.8199 epsilon: 0.0059, -0.0731 Iteration 7. ELBO: 82.2086 Pi0: 0.8150 epsilon: 0.0049, -0.0563 Iteration 8. ELBO: 82.1642 Pi0: 0.8108 epsilon: 0.0042, -0.0444 Recalculating regularizer, step 9 Iteration 9. ELBO: -20.1115 Pi0: 0.8073 epsilon: 0.0036, -102.2757 Iteration 10. ELBO: 3.6515 Pi0: 0.8006 epsilon: 0.0067, 23.7630 Iteration 11. ELBO: 9.1539 Pi0: 0.8015 epsilon: 0.0021, 5.5024 Iteration 12. ELBO: 9.7833 Pi0: 0.8017 epsilon: 0.0011, 0.6294 Iteration 13. ELBO: 9.8562 Pi0: 0.8015 epsilon: 0.0009, 0.0729 0.0009116725643496873 . Results . res_df[0] . Method RMSE Tjur R2 Gini Index . 0 Data | 0.000000 | 0.303029 | 0.974081 | . 0 LASSO | 4.892742 | 0.570087 | 0.848513 | . 0 qLash | 0.819648 | 0.423491 | 0.739561 | . res_df[1] . Method RMSE Tjur R2 Gini Index . 0 Data | 0.000000 | 0.291530 | 0.979161 | . 0 LASSO | 2.147285 | 0.515063 | 0.820613 | . 0 qLash | 1.668239 | 0.541027 | 0.854315 | . # collapse-hide fig = plt.figure(figsize = (12, 7)) ax = [fig.add_subplot(121), fig.add_subplot(122)] for i, n in enumerate(nsample): whichX = Xtest[i].copy() whichY = Ytest[i].copy() Xbeta = np.dot(whichX, beta[i]) pred_data = 1 / (1 + np.exp(-Xbeta)) ax[i].scatter(Xbeta, whichY, s = 1) Xbeta_clf = np.dot(whichX, clf[i].coef_[0]) pred_clf = 1 / (1 + np.exp(-Xbeta_clf)) ax[i].scatter(Xbeta, pred_clf, s = 1, label = &#39;Lasso&#39;) Xbeta_qL = np.dot(whichX, R[i]) pred_qL = 1 / (1 + np.exp(-Xbeta_qL)) ax[i].scatter(Xbeta, pred_qL, s = 1, label = &#39;qLash&#39;) leg = ax[i].legend(markerscale = 10) ax[i].set_xlabel(r&#39;$ sum_i X_{ni} beta_i$&#39;) ax[i].set_ylabel(r&#39;$p_n$&#39;) ax[i].set_title(f&#39;N = {nsample[i]}, P = {nvar[i]}&#39;, pad = 20) plt.tight_layout() plt.show() . .",
            "url": "https://banskt.github.io/glm-ash-notes/jupyter/2020/07/07/quasi-Laplace-mrash-logistic.html",
            "relUrl": "/jupyter/2020/07/07/quasi-Laplace-mrash-logistic.html",
            "date": " • Jul 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Comparing Quasi-Laplace approximation with true posterior",
            "content": "About . In the quasi-Laplace approximation, we apply the Laplace approximation to a regularized likelihood $ mathscr{L}_{ mathrm{reg}}( boldsymbol{ beta})$ defined as the product of the likelihood and a Gaussian regularizer, begin{equation*} mathscr{L}_{ mathrm{reg}}( boldsymbol{ beta}) triangleq p left( mathbf{y} mid mathbf{X}, boldsymbol{ beta} right) mathcal{N} left( boldsymbol{ beta} mid mathbf{0}, mathbf{ Lambda}^{-1} right) propto mathcal{N} left( boldsymbol{ beta} mid mathbf{m}, mathbf{S} right) end{equation*} such that the mode of the regularized likelihood is near the mode of the posterior. . Likelihood: $p left( mathbf{y} mid mathbf{X}, boldsymbol{ beta} right)$ | Regularized likelihood: $p left( mathbf{y} mid mathbf{X}, boldsymbol{ beta} right) mathcal{N} left( boldsymbol{ beta} mid mathbf{0}, mathbf{ Lambda}^{-1} right)$ | Prior: $p left( boldsymbol{ beta} mid g right)$ | Posterior: $p left( mathbf{y} mid mathbf{X}, boldsymbol{ beta} right) p left( boldsymbol{ beta} mid g right)$ | Marginal likelihood: $p left( mathbf{y} mid mathbf{X} right) = int p left( mathbf{y} mid mathbf{X}, boldsymbol{ beta} right) p left( boldsymbol{ beta} mid g right) d boldsymbol{ beta}$ | . Here, $g$ is the mixture of Gaussians with known variances but unknown mixture coefficients. The precision matrix $ mathbf{ Lambda}$ is defined as a diagonal matrix, $ mathbf{ Lambda} triangleq mathrm{diag} left( boldsymbol{ lambda} right)$, whose elements $ lambda_j$ are roughly set to some expected value to ensure that the regularized likelihood is centered at the mode of the posterior. . The quasi-Laplace approximation is helpful in cases where the marginal likelihood (and therefore, the posterior) is not analytically tractable, for example in logistic regression. . #collapse_hide import numpy as np np.set_printoptions(precision = 4, suppress=True) from scipy import optimize import matplotlib.pyplot as plt from matplotlib import cm from matplotlib import ticker as plticker from mpl_toolkits.axes_grid1 import make_axes_locatable import sys sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;kelly&#39;, dpi = 72) . . Generate toy data . Let us consider a logistic model with sparse coefficients, so that the number of causal variables is much less than the number of variables nvar in the model. This is ensured by sampling the betas from a Gaussian mixture prior with variances given by $ sigma_k^2$ (sigmak2), where the numbers of components is given by nGcomp and the mixture coefficients given by probk, the sparsity of which is controlled by the variable sparsity that specifies the mixture coefficient of the component $ mathcal{N}(0, 0)$ (or the delta function). . nsample = 20 nvar = 30 nGcomp = 3 sparsity = 0.8 prior_strength = 5 num_inf = 1e4 # a large number for 1/sigma_k^2 when sigma_k^2 = 0 probk = np.zeros(nGcomp) probk[0] = sparsity probk[1:(nGcomp - 1)] = (1 - sparsity) / (nGcomp - 1) probk[nGcomp - 1] = 1 - np.sum(probk) sigmak2 = np.array([prior_strength * np.square(np.power(2, (i)/nGcomp) - 1) for i in range(nGcomp)]) . The data is generated from Bernoulli trials (equivalent to binomial distribution with $n=1$ and $p= sigma( mathbf{X} boldsymbol{ beta})$, where $ sigma( cdot)$ is the logistic function. $ mathbf{X}$ is centered and scaled such that for each variable $j$, the variance is $ mathrm{var}( mathbf{x}_j) = 1$. . # collapse-hide def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) Xstd = Xnorm / np.std(Xnorm, axis = 0) return Xstd def logistic_data(X, beta): Xbeta = np.dot(X, beta) pred = 1 / (1 + np.exp(-Xbeta)) Y = np.random.binomial(1, pred) return Y X = np.random.rand(nsample * nvar).reshape(nsample, nvar) X = standardize(X) gammajk = np.random.multinomial(1, probk, size = nvar) beta = np.zeros(nvar) for j in range(nvar): if gammajk[j, 0] != 1: kidx = np.where(gammajk[j, :] == 1)[0][0] kstd = np.sqrt(sigmak2[kidx]) beta[j] = np.random.normal(loc = 0., scale = kstd) ncausal = beta[beta != 0].shape[0] betavar = np.var(beta[beta != 0]) Y = logistic_data(X, beta) . . Let us have a look at the generated data. . # collapse-hide print(f&#39;There are {ncausal} non-zero coefficients with variance {betavar:.4f}&#39;) fig = plt.figure(figsize = (6,6)) ax1 = fig.add_subplot(111) Xbeta = np.dot(X, beta) pred = 1 / (1 + np.exp(-Xbeta)) ax1.scatter(Xbeta, Y, s = 10) ax1.scatter(Xbeta, pred, s = 10) ax1.set_xlabel(r&#39;$ sum_i X_{ni} beta_i$&#39;) ax1.set_ylabel(r&#39;$Y_n$&#39;) plt.show() . . There are 5 non-zero coefficients with variance 0.5920 . True posterior vs quasi-Laplace posterior . We select two causal variables (with maximum effect size) and fix all the others to optimum values to understand how the likelihood and posterior depends on these two chosen variables. To avoid the sum over the indicator variables, we use the joint prior $p left( boldsymbol{ beta}, boldsymbol{ gamma} mid g right)$. . Some useful function definitions: . # collapse-hide def get_log_likelihood(Y, X, beta): Xbeta = np.dot(X, beta) logL = np.sum(Y * Xbeta - np.log(1 + np.exp(Xbeta))) return logL def get_log_prior(beta, gammajk, probk, sigmak2): logprior = 0 for j, b in enumerate(beta): k = np.where(gammajk[j, :] == 1)[0][0] logprior += np.log(probk[k]) if k &gt; 0: logprior += - 0.5 * (np.log(2 * np.pi) + np.log(sigmak2[k]) + b * b / sigmak2[k]) return logprior def plot_contours(ax, X, Y, Z, beta, norm, cstep = 10, zlabel = &quot;&quot;): zmin = np.min(Z) - 1 * np.std(Z) zmax = np.max(Z) + 1 * np.std(Z) ind = np.unravel_index(np.argmax(Z, axis=None), Z.shape) levels = np.linspace(zmin, zmax, 200) clevels = np.linspace(zmin, zmax, 20) cmap = cm.YlOrRd_r if norm: cset1 = ax.contourf(X, Y, Z, levels, norm = norm, cmap=cm.get_cmap(cmap, len(levels) - 1)) else: cset1 = ax.contourf(X, Y, Z, levels, cmap=cm.get_cmap(cmap, len(levels) - 1)) cset2 = ax.contour(X, Y, Z, clevels, colors=&#39;k&#39;) for c in cset2.collections: c.set_linestyle(&#39;solid&#39;) ax.set_aspect(&quot;equal&quot;) ax.scatter(beta[0], beta[1], color = &#39;blue&#39;, s = 100) ax.scatter(X[ind[1]], Y[ind[0]], color = &#39;k&#39;, s = 100) divider = make_axes_locatable(ax) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.2) cbar = plt.colorbar(cset1, cax=cax) ytickpos = np.arange(int(zmin / cstep) * cstep, zmax, cstep) cbar.set_ticks(ytickpos) if zlabel: cax.set_ylabel(zlabel) #loc = plticker.AutoLocator() #ax.xaxis.set_major_locator(loc) #ax.yaxis.set_major_locator(loc) def log_likelihood(beta, X, Y, L): nvar = beta.shape[0] Xbeta = np.dot(X, beta) ## Function llb = np.sum(np.dot(Y, Xbeta) - np.log(1 + np.exp(Xbeta))) #reg = 0.5 * np.sum(np.log(L)) - 0.5 * np.einsum(&#39;i,i-&gt;i&#39;, np.square(beta), L) reg = - 0.5 * np.einsum(&#39;i,i-&gt;i&#39;, np.square(beta), L) loglik = llb + reg ## Gradient pred = 1 / (1 + np.exp(-Xbeta)) der = np.einsum(&#39;i,ij-&gt;j&#39;, (Y - pred), X) - np.multiply(beta, L) return -loglik, -der def precisionLL(X, beta, L): nvar = X.shape[1] Xbeta = np.einsum(&#39;i,ji-&gt;j&#39;, beta, X) pred = 1 / (1 + np.exp(-Xbeta)) hess = - np.einsum(&#39;i,i,ij,ik-&gt;jk&#39;, pred, (1 - pred), X, X) hess[np.diag_indices(nvar)] -= L return -hess def get_mS(X, Y, beta0, L): nvar = X.shape[1] args = X, Y, L gmode = optimize.minimize(log_likelihood, beta0, args=args, method=&#39;L-BFGS-B&#39;, jac=True, bounds=None, options={&#39;maxiter&#39;: 20000000, &#39;maxfun&#39;: 20000000, &#39;ftol&#39;: 1e-9, &#39;gtol&#39;: 1e-9 #&#39;disp&#39;: True }) M = gmode.x Sinv = precisionLL(X, M, L) return M, Sinv def get_qL_log_posterior(beta, L, M, Sinv, logdetSinv, logprior): blessM = beta - M bMSbM = np.dot(blessM.T, np.dot(Sinv, blessM)) bLb = np.einsum(&#39;i, i&#39;, np.square(beta), L) logdetLinv = - np.sum(np.log(L)) logposterior = 0.5 * (logdetSinv + logdetLinv - bMSbM + bLb) logposterior += logprior return logposterior . . We calculate the likelihood, prior, true posterior and the quasi-Laplace posterior. Note that the posteriors are not normalized. We apply quasi-Laplace (QL) approximation with some $ mathbf{ Lambda}$ and show QL posterior distribution, which is given by begin{equation*} p left( mathbf{y} mid mathbf{X}, boldsymbol{ beta} right) p left( boldsymbol{ beta} mid g right) propto frac{ mathcal{N} left( boldsymbol{ beta} mid mathbf{m}, mathbf{S} right) p left( boldsymbol{ beta} mid g right) }{ mathcal{N} left( boldsymbol{ beta} mid mathbf{0}, mathbf{ Lambda}^{-1} right) } end{equation*} Here, we assume that we know $ mathbf{ Lambda}$. In reality, we will not know $ mathbf{ Lambda}$ but will have to learn it from the data or make some educated guess from the prior choice. . # collapse-hide bchoose = np.argsort(abs(beta))[-2:] nplotx = 20 nploty = 20 b1min = -5 b1max = 5 b2min = -5 b2max = 5 beta1 = np.linspace(b1min, b1max, nplotx) beta2 = np.linspace(b2min, b2max, nploty) logL = np.zeros((nploty, nplotx)) logPr = np.zeros((nploty, nplotx)) logPs = np.zeros((nploty, nplotx)) logQL = np.zeros((nploty, nplotx)) thisbeta = beta.copy() mask = np.ones(nvar, bool) mask[bchoose] = False true_pi = np.sum(gammajk, axis = 0) / np.sum(gammajk) #reg = 1 / np.einsum(&#39;i,i&#39;, true_pi, sigmak2) #regL = np.repeat(reg, nvar) regL = np.repeat(num_inf, nvar) for j, b in enumerate(beta): k = np.where(gammajk[j, :] == 1)[0][0] if k &gt; 0: regL[j] = 1 / sigmak2[k] M, Sinv = get_mS(X, Y, beta, regL) sgndetSinv, logdetSinv = np.linalg.slogdet(Sinv) for i, b1 in enumerate(beta1): for j, b2 in enumerate(beta2): thisbeta[bchoose] = np.array([b1, b2]) logL[j, i] = get_log_likelihood(Y, X, thisbeta) logPr[j, i] = get_log_prior(thisbeta, gammajk, probk, sigmak2) logQL[j, i] = get_qL_log_posterior(thisbeta, regL, M, Sinv, logdetSinv, logPr[j, i]) logPs = logL + logPr . . Here, we plot the contour maps. The x and y-axis show the two coefficients $ beta_1$ and $ beta_2$, which we chose to vary. The blue dot shows the coordinates of the true values of $ { beta_1, beta_2 }$ and the black dot shows the maximum of the log probabilities. Note how the non-Gaussian true posterior is now approximated by a Gaussian QL posterior. . fig = plt.figure(figsize = (12, 8)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) norm = cm.colors.Normalize(vmin=np.min(logPs), vmax=np.max(logPs)) plot_contours(ax1, beta1, beta2, logL, beta[bchoose], None, cstep = 50, zlabel = &quot;Log Likelihood&quot;) plot_contours(ax2, beta1, beta2, logPr, beta[bchoose], None, cstep = 20, zlabel = &quot;Log Prior&quot;) plot_contours(ax3, beta1, beta2, logPs, beta[bchoose], None, cstep = 50, zlabel = &quot;Log Posterior&quot;) plot_contours(ax4, beta1, beta2, logQL, beta[bchoose], None, cstep = 40, zlabel = &quot;Log QL Posterior&quot;) plt.tight_layout() plt.show() . Effect of regularizer . Let us assume that we do not know $ mathbf{ Lambda}$ and all $ lambda_j$&#39;s are equal. Here we look at how the QL posterior changes with varying $ beta_2$ for different values of $ lambda_j$. . Here I define a function for calculating the QL posterior and true posterior for changing the coefficients of a single variable: . # collapse-hide def get_logQL_logPs_single_variable(X, Y, beta, regvar, betavar, bidx, regL, gammajk, probk, sigmak2): nvar = beta.shape[0] nplotrv = regvar.shape[0] nplotx = betavar.shape[0] logL = np.zeros(nplotx) logPr = np.zeros(nplotx) logPs = np.zeros(nplotx) logQL = np.zeros((nplotrv, nplotx)) thisbeta = beta.copy() thisL = regL.copy() for j, b2 in enumerate(betavar): thisbeta[bidx] = b2 logL[j] = get_log_likelihood(Y, X, thisbeta) logPr[j] = get_log_prior(thisbeta, gammajk, probk, sigmak2) logPs = logL + logPr for i, r1 in enumerate(regvar): thisL = np.repeat(r1, nvar) #thisL[bidx] = r1 M, Sinv = get_mS(X, Y, beta, thisL) sgndetSinv, logdetSinv = np.linalg.slogdet(Sinv) for j, b2 in enumerate(betavar): thisbeta[bidx] = b2 logQL[i, j] = get_qL_log_posterior(thisbeta, thisL, M, Sinv, logdetSinv, logPr[j]) return logPs, logQL . . And then look at the posteriors for $ beta_2$. . #collapse-hide nplotx_rv = 200 nplotrv = 4 bmin = -10 bmax = 15 rvmin = 1 rvmax = 10 bidx = bchoose[1] fig = plt.figure(figsize = (8, 8)) ax1 = fig.add_subplot(111) betavals = np.linspace(bmin, bmax, nplotx_rv) regvals = np.linspace(rvmin, rvmax, nplotrv) logPs_rv, logQL_rv = get_logQL_logPs_single_variable(X, Y, beta, regvals, betavals, bidx, regL, gammajk, probk, sigmak2) ax1.plot(betavals, logPs_rv, lw = 3, zorder = 2, label = &#39;True Posterior&#39;) ax1.scatter(betavals[np.argmax(logPs_rv)], logPs_rv[np.argmax(logPs_rv)], s = 40, zorder = 10, color = &#39;black&#39;) for i, r1 in enumerate(regvals): ax1.plot(betavals, logQL_rv[i, :], lw = 2, zorder = 5, label = f&#39;$ lambda =${r1:.2f}&#39;) ix = np.argmax(logQL_rv[i, :]) ax1.scatter(betavals[ix], logQL_rv[i, ix], s = 40, zorder = 10, color = &#39;black&#39;) ax1.axvline(beta[bidx], ls = &#39;dotted&#39;, zorder = 1) ax1.legend(handlelength = 1.5) ax1.set_xlabel(r&#39;$ beta$&#39;) ax1.set_ylabel(&#39;Log Posterior&#39;) plt.tight_layout() plt.show() . . What happens to the QL posterior for other variables? Let us look at every $ beta_j$ and their individual maximum posterior, while all others are kept fixed at optimum values. Here, I have arranged the $ beta_j$ in ascending order for easy viewing. . #collapse-hide bidx_sorted = np.argsort(beta) bidx_sorted_nz = bidx_sorted #bidx_sorted[beta[bidx_sorted]!=0] maxQL = np.zeros((nplotrv, len(bidx_sorted_nz))) maxPs = np.zeros(len(bidx_sorted_nz)) for i, bidx in enumerate(bidx_sorted_nz): _logPs, _logQL = get_logQL_logPs_single_variable(X, Y, beta, regvals, betavals, bidx, regL, gammajk, probk, sigmak2) maxPs[i] = betavals[np.argmax(_logPs)] for j in range(len(regvals)): maxQL[j, i] = betavals[np.argmax(_logQL[j, :])] fig = plt.figure(figsize = (16, 8)) ax1 = fig.add_subplot(111) xvals = np.arange(len(bidx_sorted_nz)) ax1.plot(xvals, maxPs, lw = 2, zorder = 2, label = &#39;True Posterior&#39;) for i, r1 in enumerate(regvals): ax1.plot(xvals, maxQL[i, :], lw = 2, zorder = 5, label = f&#39;$ lambda =${r1:.2f}&#39;) ax1.scatter(xvals, maxPs, s = 20, zorder = 1) for i, r1 in enumerate(regvals): ax1.scatter(xvals, maxQL[i, :], s = 20, zorder = 1) ax1.scatter(xvals, beta[bidx_sorted_nz], s = 80, zorder = 10, color = &#39;maroon&#39;, label = &#39;Input&#39;) ax1.legend(handlelength = 1.5) ax1.set_xlabel(r&#39;Index of $ beta$&#39;) ax1.set_ylabel(r&#39;$ beta$ at maximum log posterior&#39;) ax1.set_xticks(xvals) ax1.set_xticklabels([f&#39;{idx}&#39; for idx in bidx_sorted_nz]) plt.tight_layout() plt.show() . .",
            "url": "https://banskt.github.io/glm-ash-notes/jupyter/2020/06/22/intuition-for-quasi-Laplace.html",
            "relUrl": "/jupyter/2020/06/22/intuition-for-quasi-Laplace.html",
            "date": " • Jun 22, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://banskt.github.io/glm-ash-notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://banskt.github.io/glm-ash-notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}