{
  
    
        "post0": {
            "title": "Logistic model quasi-Laplace with Mr.Ash",
            "content": "About . This is a proof of concept implementation of quasi-Laplace approximation with Mr.Ash. The quasi-Laplace approximation introduces a regularizer on the likelihood and approximates the posterior with a Gaussian distribution. This approach can be used for generalized linear model with any link function. . Bayesian logistic regression with a Gaussian prior (ridge regression) uses the Laplace approximation to approximate the posterior with a Gaussian distribution. The variational treatment of logistic model by Jordan and Jaakkola (2000) also leads to a Gaussian approximation of the posterior distribution. The greater flexibility of the variational approximation leads to improved accuracy compared to Laplace method. . Here, I compare the quasi-Laplace approach with cross-validated Lasso ($L_1$ penalty). I used the $R^2$ statistic by Tjur1 to quantify the quality of classification, and the Gini index2 to quantify the sparsity of the coefficients. . Note: Coefficients are generally underestimated by QL irrespective of the choice of regularizer. Lasso overestimates the coefficients. I found that the optimization depends on the choice of Gaussian mixture family. . #collapse_hide import numpy as np np.set_printoptions(precision = 4, suppress=True) import pandas as pd from scipy import optimize from scipy import stats from sklearn import linear_model import matplotlib.pyplot as plt from matplotlib import cm from matplotlib import ticker as plticker from mpl_toolkits.axes_grid1 import make_axes_locatable import sys sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 72) . . Generate toy data . Let us consider a logistic model with sparse coefficients, so that the number of causal variables ncausal is much less than the number of variables nvar in the model. This is ensured by sampling the betas from a Gaussian mixture prior of nGcomp components with variances given by $ sigma_k^2$ (sigmak2) and unknown mixture coefficients. The sparsity of the initialization is controlled by the variable sparsity that specifies the mixture coefficient of the component $ mathcal{N}(0, 0)$ (or the delta function). We are particularly interested for systems where the number of samples nsample is less than nvar. Validation is performed on separate test dataset with ntest samples. . nsample = [50, 200] nvar = [100, 100] ncausal = 5 nGcomp = 10 sparsity = 0.95 prior_variance = 5 ntest = 2000 hg2 = 0.8 sigmak2 = np.array([prior_variance * np.square(np.power(2, (i)/nGcomp) - 1) for i in range(nGcomp)]) . The data is generated from a liability threshold model such that the explanatory variables explain only a given fraction hg2 of the response variable. $ mathbf{X}$ is centered and scaled such that for each variable $j$, the variance is $ mathrm{var}( mathbf{x}_j) = 1$. . # collapse-hide def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) Xstd = Xnorm / np.std(Xnorm, axis = 0) return Xstd def logistic_data(X, beta): Xbeta = np.dot(X, beta) pred = 1 / (1 + np.exp(-Xbeta)) Y = np.random.binomial(1, pred) return Y, pred def liability_model(X, beta, prevalence = 0.5): hg2 = np.sum(np.square(beta)) eff = np.dot(X, beta) rand_std = np.sqrt(1 - hg2) rand_eff = np.random.normal(0, rand_std, X.shape[0]) liability = eff + rand_eff cutoff = stats.norm.ppf(1 - prevalence) cases = np.where(liability &gt;= cutoff)[0] ctrls = np.where(liability &lt; cutoff)[0] Y = np.zeros(X.shape[0]) Y[cases] = 1 return Y, liability def tjur_R2(X, Y, beta): Xbeta = np.dot(X, beta) pred = 1 / (1 + np.exp(-Xbeta)) R2 = np.mean(pred[Y == 1]) - np.mean(pred[Y == 0]) return R2 def gini(x): n = len(x) xabs = abs(x) xsort = xabs[np.argsort(xabs)] t1 = xsort / np.sum(xabs) t2 = (n - np.arange(n) - 1 + 0.5) / n gini = 1 - 2 * np.sum(t1 * t2) return gini def simulate_data(nsample, nvar, ncausal, ntest, hg2): X = np.random.rand(nsample * nvar).reshape(nsample, nvar) X = standardize(X) Xtest = np.random.rand(ntest * nvar).reshape(ntest, nvar) Xtest = standardize(Xtest) beta = np.zeros(nvar) causal_idx = np.random.choice(nvar, ncausal, replace = False) beta[causal_idx] = np.random.normal(loc = 0., scale = 1., size = ncausal) beta = beta * np.sqrt(hg2 / np.sum(np.square(beta))) Y, Y_liab = liability_model(X, beta) Ytest, Ytest_liab = liability_model(Xtest, beta) #Y, Y_liab = logistic_data(X, beta) #Ytest, Ytest_liab = logistic_data(Xtest, beta) return X, Y, beta, Xtest, Ytest, Y_liab, Ytest_liab . . Let us have a look at the generated data. . # collapse-hide X = [None for n in nsample] Xtest = [None for n in nsample] Y = [None for n in nsample] Ytest = [None for n in nsample] Y_liab = [None for n in nsample] Ytest_liab = [None for n in nsample] beta = [None for n in nsample] for i, n in enumerate(nsample): X[i], Y[i], beta[i], Xtest[i], Ytest[i], Y_liab[i], Ytest_liab[i] = simulate_data(n, nvar[i], ncausal, ntest, hg2) print(f&#39;There are {ncausal} non-zero coefficients with heritability {hg2:.4f}&#39;) fig = plt.figure(figsize = (12,6)) ax = [fig.add_subplot(121), fig.add_subplot(122)] for i, n in enumerate(nsample): Xbeta = np.dot(X[i], beta[i]) pred = 1 / (1 + np.exp(-Xbeta)) ax[i].scatter(Xbeta, Y[i], s = 10) ax[i].scatter(Xbeta, pred, s = 10) ax[i].set_xlabel(r&#39;$ sum_i X_{ni} beta_i$&#39;) ax[i].set_ylabel(r&#39;$Y_n$&#39;) plt.tight_layout() plt.show() . . There are 5 non-zero coefficients with heritability 0.8000 . Implementation of quasi-Laplace with variational approximation . See Latex notes for details of the theory. Current implementation focus on readability and debug. Computational efficiency has been ignored. . Warning: ELBO decreasing with parameter update. There must be an error in theory or implementation. Debug! . # collapse-hide def regularized_log_likelihood(beta, X, Y, L): nvar = beta.shape[0] Xbeta = np.dot(X, beta) ## Function llb = np.sum(np.dot(Y, Xbeta) - np.log(1 + np.exp(Xbeta))) reg = - 0.5 * np.einsum(&#39;i,i-&gt;i&#39;, np.square(beta), L) loglik = llb + reg ## Gradient pred = 1 / (1 + np.exp(-Xbeta)) der = np.einsum(&#39;i,ij-&gt;j&#39;, (Y - pred), X) - np.multiply(beta, L) return -loglik, -der def precisionLL(X, beta, L): nvar = X.shape[1] Xbeta = np.einsum(&#39;i,ji-&gt;j&#39;, beta, X) pred = 1 / (1 + np.exp(-Xbeta)) Sinv = np.einsum(&#39;i,i,ij,ik-&gt;jk&#39;, pred, (1 - pred), X, X) Sinv[np.diag_indices(nvar)] += L return Sinv def get_regularized_mode(X, Y, beta0, L): nsample, nvar = X.shape args = X, Y, L gmode = optimize.minimize(regularized_log_likelihood, beta0, args=args, method=&#39;L-BFGS-B&#39;, jac=True, bounds=None, options={&#39;maxiter&#39;: 20000000, &#39;maxfun&#39;: 20000000, &#39;ftol&#39;: 1e-9, &#39;gtol&#39;: 1e-9 #&#39;disp&#39;: True }) return gmode.x def get_elbo(alpha, mu, s2, pi, sigmak2, XPX, SinvM, M, L, logdetS): nvar, nGcomp = alpha.shape Ebeta = np.sum(np.multiply(alpha, mu)[:, 1:], axis = 1) mu2 = np.square(mu) logdetL = np.sum(np.log(L)) t0 = - 0.5 * (logdetL + logdetS + np.dot(M.T, SinvM)) t1 = - 0.5 * np.dot(Ebeta.T, np.dot(XPX, Ebeta)) t2 = np.dot(Ebeta.T, SinvM) t3 = 0 t4 = 0 for j in range(nvar): Ebeta2j = 0 for k in range(nGcomp): mom2jk = mu2[j, k] + s2[j, k] Ebeta2j += alpha[j, k] * mom2jk t4jk = 0 if alpha[j, k] != 0: t4jk += np.log(pi[k]) t4jk += - np.log(alpha[j, k]) if k &gt; 0: t4jk += 0.5 * np.log(s2[j, k]) t4jk += - 0.5 * np.log(sigmak2[k]) t4jk += - 0.5 * mom2jk / sigmak2[k] t4jk += 0.5 t4jk *= alpha[j, k] t4 += t4jk t3 += 0.5 * XPX[j, j] * (Ebeta[j] * Ebeta[j] - Ebeta2j) elbo = t1 + t2 + t3 + t4 return elbo def ash_qL_regularizer(X, Y, binit, L, mode_optim = True): nsample, nvar = X.shape if mode_optim: M = get_regularized_mode(X, Y, binit, L) else: M = binit.copy() Sinv = precisionLL(X, M, L) S = np.linalg.inv(Sinv) sgndetS, logdetS = np.linalg.slogdet(S) XPX = Sinv.copy() XPX[np.diag_indices(nvar)] -= L return M, S, Sinv, logdetS, XPX def ash_qL_pi_init(nGcomp, pi0, sparse = True): &#39;&#39;&#39; By default, pi is initialized with sparsity, i.e. pi[0] &gt; pi[j], for j &gt; 0 &#39;&#39;&#39; pi = np.zeros(nGcomp) pi[0] = pi0 pi[1:(nGcomp - 1)] = (1 - pi0) / (nGcomp - 1) pi[nGcomp - 1] = 1 - np.sum(pi) if not sparse: pi = np.repeat(1 / nGcomp, nGcomp) return pi def ash_logistic_qL(X, Y, nGcomp, sigmak2, binit, Linit, a_dirich = None, maxiter = 1000, tol = 1e-10, pi0 = 0.8, sparse_init = True, reg_step = 1, reg_tol = 0.1, debug = True, debug_step = 10): &#39;&#39;&#39; Current focus is on readability and debug. &#39;&#39;&#39; nsample, nvar = X.shape # default dirichlet prior on pi if a_dirich is None: a_dirich = np.repeat(1., nGcomp) a_dirich[0] = 1. # soft max at pi[0] &#39;&#39;&#39; Initialize &#39;&#39;&#39; L = Linit.copy() M, S, Sinv, logdetS, XPX = ash_qL_regularizer(X, Y, binit, L) SinvM = np.dot(Sinv, M) SinvRtj = np.dot(Sinv, M) - np.multiply(np.diag(Sinv), M) pi = ash_qL_pi_init(nGcomp, pi0, sparse = sparse_init) alpha = np.repeat(np.array([pi]), nvar, axis = 0) logalpha = np.log(alpha) s2 = np.zeros((nvar, nGcomp)) s2[:, 1:] = 1 / (np.diag(XPX).reshape(-1, 1) + 1 / sigmak2[1:]) mu = np.multiply(s2, (SinvM - SinvRtj).reshape(-1,1)) R = np.sum(np.multiply(alpha, mu)[:, 1:], axis = 1) &#39;&#39;&#39; Iteration &#39;&#39;&#39; elbo_old = get_elbo(alpha, mu, s2, pi, sigmak2, XPX, SinvM, M, L, logdetS) pi_old = pi.copy() pi_reg = pi.copy() if debug: print(f&quot;Starting ELBO: {elbo_old:.4f} Pi0: {pi[0]:.4f}&quot;) for it in range(maxiter): # E-step for j in range(nvar): SinvRtj = np.sum(np.multiply(Sinv[:, j], R[:])) - Sinv[j, j] * R[j] s2[j, 0] = 0. mu[j, 0] = 0. logalpha[j, 0] = np.log(pi[0]) for k in range(1, nGcomp): s2[j, k] = 1 / (XPX[j, j] + (1 / sigmak2[k])) mu[j, k] = s2[j, k] * (SinvM[j] - SinvRtj) ssf = np.log(s2[j, k] / sigmak2[k]) ssr = mu[j, k] * mu[j, k] / s2[j, k] logalpha[j, k] = np.log(pi[k]) + 0.5 * ssf + 0.5 * ssr maxL = np.max(logalpha[j, :]) alpha[j, :] = np.exp(logalpha[j, :] - maxL) alpha[j, :] /= np.sum(alpha[j, :]) R[j] = np.sum(np.multiply(alpha[j, 1:], mu[j, 1:])) # M-step bk = np.sum(alpha, axis = 0) + a_dirich pi = bk / np.sum(bk) # Conditional M-step if (it + 1) % reg_step == 0: eps_reg = max([abs(x - y) for x, y in zip(pi, pi_reg)]) if eps_reg &gt; reg_tol: print(f&quot;Recalculating regularizer, step {it}&quot;) L = 1 / np.sum(alpha * sigmak2, axis = 1) M, S, Sinv, logdetS, XPX = ash_qL_regularizer(X, Y, R, L) SinvM = np.dot(Sinv, M) pi_reg = pi.copy() elbo = get_elbo(alpha, mu, s2, pi, sigmak2, XPX, SinvM, M, L, logdetS) eps_elbo = elbo - elbo_old eps_pi = max([abs(x - y) for x, y in zip(pi, pi_old)]) if debug and (it % debug_step == 0): print(f&quot;Iteration {it}. ELBO: {elbo:.4f} Pi0: {pi[0]:.4f} epsilon: {eps_pi:.4f}, {eps_elbo:.4f}&quot;) &#39;&#39;&#39; Check termination, continue otherwise. &#39;&#39;&#39; if eps_pi &lt; tol: print(eps_pi) break &#39;&#39;&#39; Keep this step in memory. Use deep copy to avoid in place substitutions. &#39;&#39;&#39; pi_old = pi.copy() alpha_old = alpha.copy() mu_old = mu.copy() s2_old = s2.copy() R_old = R.copy() elbo_old = elbo return pi, alpha, mu, s2, R, L . . Hold the result summary statistic in a data frame . # collapse-hide res_df = [pd.DataFrame(columns = [&#39;Method&#39;, &#39;RMSE&#39;, &#39;Tjur R2&#39;, &#39;Gini Index&#39;]) for n in nsample] data_Xbeta = [np.dot(Xtest[i], beta[i]) for i in range(len(nsample))] data_tjur = [tjur_R2(Xtest[i], Ytest[i], beta[i]) for i in range(len(nsample))] data_rmse = [np.sqrt(np.mean(np.square(data_Xbeta[i] - np.dot(Xtest[i], beta[i])))) for i in range(len(nsample))] data_gini = [gini(beta[i]) for i in range(len(nsample))] for i in range(len(nsample)): res_df[i] = res_df[i].append(pd.DataFrame([[&#39;Data&#39;, data_rmse[i], data_tjur[i], data_gini[i]]], columns = res_df[i].columns)) . . Fitting . Lasso | # collapse-hide clf = [linear_model.LogisticRegressionCV(cv = 5, penalty=&#39;l1&#39;, solver=&#39;saga&#39;, fit_intercept = False, max_iter = 5000) for n in nsample] for i, n in enumerate(nsample): clf[i].fit(X[i], Y[i]) clf_Xbeta = [np.dot(Xtest[i], clf[i].coef_[0]) for i in range(len(nsample))] clf_tjur = [tjur_R2(Xtest[i], Ytest[i], clf[i].coef_[0]) for i in range(len(nsample))] clf_rmse = [np.sqrt(np.mean(np.square(clf_Xbeta[i] - np.dot(Xtest[i], beta[i])))) for i in range(len(nsample))] clf_gini = [gini(clf[i].coef_[0]) for i in range(len(nsample))] for i in range(len(nsample)): res_df[i] = res_df[i].append(pd.DataFrame([[&#39;LASSO&#39;, clf_rmse[i], clf_tjur[i], clf_gini[i]]], columns = res_df[i].columns)) . . QL Ash | # collapse-hide pi = [None for n in nsample] alpha = [None for n in nsample] mu = [None for n in nsample] s2 = [None for n in nsample] R = [None for n in nsample] L = [None for n in nsample] #probk = ash_qL_pi_init(nGcomp, sparsity) for i in range(len(nsample)): gold_reg = np.repeat(1e4, nvar[i]) for j, b in enumerate(beta[i]): if b != 0: gold_reg[j] = 1 / prior_variance fix_reg = np.repeat(1 / sigmak2[nGcomp - 1], nvar[i]) #fix_reg = np.repeat( 1 / np.dot(probk, sigmak2), nvar[i]) #fix_reg = np.repeat(1e2, nvar[i]) binit = np.zeros(nvar[i]) Linit = fix_reg pi[i], alpha[i], mu[i], s2[i], R[i], L[i] = ash_logistic_qL(X[i], Y[i], nGcomp, sigmak2, binit, Linit, tol = 1e-3, debug_step = 1, maxiter = 1000, reg_step = 10, pi0 = sparsity, sparse_init = True) qLash_Xbeta = [np.dot(Xtest[i], R[i]) for i in range(len(nsample))] qLash_rmse = [np.sqrt(np.mean(np.square(qLash_Xbeta[i] - np.dot(Xtest[i], beta[i])))) for i in range(len(nsample))] qLash_r2 = [tjur_R2(Xtest[i], Ytest[i], R[i]) for i in range(len(nsample))] qLash_gini = [gini(R[i]) for i in range(len(nsample))] for i in range(len(nsample)): res_df[i] = res_df[i].append(pd.DataFrame([[&#39;qLash&#39;, qLash_rmse[i], qLash_r2[i], qLash_gini[i]]], columns = res_df[i].columns)) . . Starting ELBO: -0.9889 Pi0: 0.9500 Iteration 0. ELBO: -4.3773 Pi0: 0.8794 epsilon: 0.0706, -3.3884 Iteration 1. ELBO: -3.6696 Pi0: 0.8234 epsilon: 0.0560, 0.7077 Iteration 2. ELBO: -3.7613 Pi0: 0.7778 epsilon: 0.0457, -0.0918 Iteration 3. ELBO: -3.9672 Pi0: 0.7394 epsilon: 0.0383, -0.2058 Iteration 4. ELBO: -4.1725 Pi0: 0.7065 epsilon: 0.0329, -0.2054 Iteration 5. ELBO: -4.3540 Pi0: 0.6778 epsilon: 0.0287, -0.1815 Iteration 6. ELBO: -4.5093 Pi0: 0.6523 epsilon: 0.0255, -0.1554 Iteration 7. ELBO: -4.6416 Pi0: 0.6295 epsilon: 0.0228, -0.1322 Iteration 8. ELBO: -4.7545 Pi0: 0.6088 epsilon: 0.0207, -0.1129 Recalculating regularizer, step 9 Iteration 9. ELBO: -19.6318 Pi0: 0.5900 epsilon: 0.0188, -14.8773 Iteration 10. ELBO: -8.2688 Pi0: 0.6095 epsilon: 0.0195, 11.3631 Iteration 11. ELBO: -7.1470 Pi0: 0.6200 epsilon: 0.0106, 1.1217 Iteration 12. ELBO: -6.6160 Pi0: 0.6246 epsilon: 0.0063, 0.5310 Iteration 13. ELBO: -6.3357 Pi0: 0.6258 epsilon: 0.0056, 0.2803 Iteration 14. ELBO: -6.1770 Pi0: 0.6250 epsilon: 0.0051, 0.1587 Iteration 15. ELBO: -6.0832 Pi0: 0.6232 epsilon: 0.0047, 0.0938 Iteration 16. ELBO: -6.0264 Pi0: 0.6206 epsilon: 0.0044, 0.0568 Iteration 17. ELBO: -5.9917 Pi0: 0.6178 epsilon: 0.0042, 0.0347 Iteration 18. ELBO: -5.9708 Pi0: 0.6147 epsilon: 0.0040, 0.0209 Iteration 19. ELBO: -5.9588 Pi0: 0.6116 epsilon: 0.0038, 0.0120 Iteration 20. ELBO: -5.9525 Pi0: 0.6085 epsilon: 0.0036, 0.0062 Iteration 21. ELBO: -5.9502 Pi0: 0.6053 epsilon: 0.0034, 0.0023 Iteration 22. ELBO: -5.9505 Pi0: 0.6023 epsilon: 0.0033, -0.0003 Iteration 23. ELBO: -5.9526 Pi0: 0.5993 epsilon: 0.0031, -0.0021 Iteration 24. ELBO: -5.9560 Pi0: 0.5963 epsilon: 0.0030, -0.0033 Iteration 25. ELBO: -5.9602 Pi0: 0.5935 epsilon: 0.0029, -0.0042 Iteration 26. ELBO: -5.9649 Pi0: 0.5907 epsilon: 0.0028, -0.0047 Iteration 27. ELBO: -5.9700 Pi0: 0.5881 epsilon: 0.0027, -0.0051 Iteration 28. ELBO: -5.9753 Pi0: 0.5855 epsilon: 0.0026, -0.0053 Iteration 29. ELBO: -5.9807 Pi0: 0.5830 epsilon: 0.0025, -0.0054 Iteration 30. ELBO: -5.9862 Pi0: 0.5806 epsilon: 0.0024, -0.0054 Iteration 31. ELBO: -5.9916 Pi0: 0.5783 epsilon: 0.0023, -0.0054 Iteration 32. ELBO: -5.9970 Pi0: 0.5761 epsilon: 0.0022, -0.0054 Iteration 33. ELBO: -6.0022 Pi0: 0.5740 epsilon: 0.0021, -0.0053 Iteration 34. ELBO: -6.0074 Pi0: 0.5719 epsilon: 0.0021, -0.0052 Iteration 35. ELBO: -6.0124 Pi0: 0.5699 epsilon: 0.0020, -0.0050 Iteration 36. ELBO: -6.0173 Pi0: 0.5680 epsilon: 0.0019, -0.0049 Iteration 37. ELBO: -6.0220 Pi0: 0.5662 epsilon: 0.0018, -0.0048 Iteration 38. ELBO: -6.0266 Pi0: 0.5644 epsilon: 0.0018, -0.0046 Recalculating regularizer, step 39 Iteration 39. ELBO: -3.6178 Pi0: 0.5627 epsilon: 0.0017, 2.4089 Iteration 40. ELBO: -3.2378 Pi0: 0.5452 epsilon: 0.0175, 0.3800 Iteration 41. ELBO: -3.4584 Pi0: 0.5321 epsilon: 0.0131, -0.2205 Iteration 42. ELBO: -3.6187 Pi0: 0.5216 epsilon: 0.0105, -0.1603 Iteration 43. ELBO: -3.7316 Pi0: 0.5126 epsilon: 0.0089, -0.1130 Iteration 44. ELBO: -3.8141 Pi0: 0.5048 epsilon: 0.0078, -0.0825 Iteration 45. ELBO: -3.8766 Pi0: 0.4979 epsilon: 0.0070, -0.0626 Iteration 46. ELBO: -3.9259 Pi0: 0.4916 epsilon: 0.0063, -0.0492 Iteration 47. ELBO: -3.9657 Pi0: 0.4858 epsilon: 0.0058, -0.0399 Iteration 48. ELBO: -3.9989 Pi0: 0.4805 epsilon: 0.0053, -0.0331 Iteration 49. ELBO: -4.0269 Pi0: 0.4755 epsilon: 0.0050, -0.0281 Iteration 50. ELBO: -4.0511 Pi0: 0.4709 epsilon: 0.0046, -0.0242 Iteration 51. ELBO: -4.0722 Pi0: 0.4666 epsilon: 0.0043, -0.0210 Iteration 52. ELBO: -4.0907 Pi0: 0.4625 epsilon: 0.0041, -0.0185 Iteration 53. ELBO: -4.1071 Pi0: 0.4587 epsilon: 0.0038, -0.0165 Iteration 54. ELBO: -4.1218 Pi0: 0.4550 epsilon: 0.0036, -0.0147 Iteration 55. ELBO: -4.1351 Pi0: 0.4516 epsilon: 0.0034, -0.0132 Iteration 56. ELBO: -4.1471 Pi0: 0.4483 epsilon: 0.0033, -0.0120 Iteration 57. ELBO: -4.1580 Pi0: 0.4452 epsilon: 0.0031, -0.0109 Iteration 58. ELBO: -4.1679 Pi0: 0.4423 epsilon: 0.0029, -0.0099 Recalculating regularizer, step 59 Iteration 59. ELBO: -8.8276 Pi0: 0.4395 epsilon: 0.0028, -4.6598 Iteration 60. ELBO: -6.9208 Pi0: 0.4522 epsilon: 0.0127, 1.9068 Iteration 61. ELBO: -6.4042 Pi0: 0.4607 epsilon: 0.0085, 0.5167 Iteration 62. ELBO: -6.1438 Pi0: 0.4671 epsilon: 0.0064, 0.2603 Iteration 63. ELBO: -5.9713 Pi0: 0.4721 epsilon: 0.0050, 0.1725 Iteration 64. ELBO: -5.8493 Pi0: 0.4762 epsilon: 0.0041, 0.1219 Iteration 65. ELBO: -5.7595 Pi0: 0.4796 epsilon: 0.0034, 0.0898 Iteration 66. ELBO: -5.6911 Pi0: 0.4824 epsilon: 0.0028, 0.0684 Iteration 67. ELBO: -5.6377 Pi0: 0.4848 epsilon: 0.0024, 0.0534 Iteration 68. ELBO: -5.5951 Pi0: 0.4869 epsilon: 0.0021, 0.0426 Iteration 69. ELBO: -5.5605 Pi0: 0.4887 epsilon: 0.0018, 0.0346 Iteration 70. ELBO: -5.5321 Pi0: 0.4903 epsilon: 0.0016, 0.0284 Iteration 71. ELBO: -5.5085 Pi0: 0.4917 epsilon: 0.0014, 0.0236 Iteration 72. ELBO: -5.4886 Pi0: 0.4929 epsilon: 0.0012, 0.0199 Iteration 73. ELBO: -5.4718 Pi0: 0.4940 epsilon: 0.0011, 0.0168 Iteration 74. ELBO: -5.4574 Pi0: 0.4950 epsilon: 0.0010, 0.0144 0.0009804561652068666 Starting ELBO: -8.2905 Pi0: 0.9500 Iteration 0. ELBO: -6.2543 Pi0: 0.8939 epsilon: 0.0561, 2.0362 Iteration 1. ELBO: -6.4640 Pi0: 0.8633 epsilon: 0.0306, -0.2097 Iteration 2. ELBO: -6.8130 Pi0: 0.8430 epsilon: 0.0203, -0.3489 Iteration 3. ELBO: -7.0604 Pi0: 0.8279 epsilon: 0.0151, -0.2475 Iteration 4. ELBO: -7.2370 Pi0: 0.8158 epsilon: 0.0121, -0.1765 Iteration 5. ELBO: -7.3688 Pi0: 0.8057 epsilon: 0.0101, -0.1319 Iteration 6. ELBO: -7.4716 Pi0: 0.7971 epsilon: 0.0086, -0.1027 Iteration 7. ELBO: -7.5542 Pi0: 0.7896 epsilon: 0.0075, -0.0826 Iteration 8. ELBO: -7.6224 Pi0: 0.7829 epsilon: 0.0066, -0.0682 Recalculating regularizer, step 9 Iteration 9. ELBO: -14.9384 Pi0: 0.7770 epsilon: 0.0059, -7.3160 Iteration 10. ELBO: 8.5555 Pi0: 0.7691 epsilon: 0.0079, 23.4938 Iteration 11. ELBO: 13.7567 Pi0: 0.7549 epsilon: 0.0142, 5.2012 Iteration 12. ELBO: 14.5027 Pi0: 0.7470 epsilon: 0.0080, 0.7460 Iteration 13. ELBO: 14.5046 Pi0: 0.7430 epsilon: 0.0039, 0.0019 Iteration 14. ELBO: 14.4582 Pi0: 0.7410 epsilon: 0.0020, -0.0463 Iteration 15. ELBO: 14.4312 Pi0: 0.7398 epsilon: 0.0012, -0.0270 Iteration 16. ELBO: 14.4176 Pi0: 0.7389 epsilon: 0.0008, -0.0136 0.0008437978604595303 . Results . i = 0 print (f&#39;N = {nsample[i]}, P = {nvar[i]}&#39;) res_df[i] . N = 50, P = 100 . Method RMSE Tjur R2 Gini Index . 0 Data | 0.000000 | 0.313842 | 0.976629 | . 0 LASSO | 1.249325 | 0.414589 | 0.951332 | . 0 qLash | 0.943936 | 0.424554 | 0.615451 | . i = 1 print (f&#39;N = {nsample[i]}, P = {nvar[i]}&#39;) res_df[i] . N = 200, P = 100 . Method RMSE Tjur R2 Gini Index . 0 Data | 0.000000 | 0.287956 | 0.965828 | . 0 LASSO | 2.036362 | 0.496911 | 0.842391 | . 0 qLash | 1.630495 | 0.512297 | 0.854587 | . # collapse-hide fig = plt.figure(figsize = (12, 7)) ax = [fig.add_subplot(121), fig.add_subplot(122)] for i, n in enumerate(nsample): whichX = Xtest[i].copy() whichY = Ytest[i].copy() Xbeta = np.dot(whichX, beta[i]) pred_data = 1 / (1 + np.exp(-Xbeta)) ax[i].scatter(Xbeta, whichY, s = 1) Xbeta_clf = np.dot(whichX, clf[i].coef_[0]) pred_clf = 1 / (1 + np.exp(-Xbeta_clf)) ax[i].scatter(Xbeta, pred_clf, s = 1, label = &#39;Lasso&#39;) Xbeta_qL = np.dot(whichX, R[i]) pred_qL = 1 / (1 + np.exp(-Xbeta_qL)) ax[i].scatter(Xbeta, pred_qL, s = 1, label = &#39;qLash&#39;) leg = ax[i].legend(markerscale = 10) ax[i].set_xlabel(r&#39;$ sum_i X_{ni} beta_i$&#39;) ax[i].set_ylabel(r&#39;$p_n$&#39;) ax[i].set_title(f&#39;N = {nsample[i]}, P = {nvar[i]}&#39;, pad = 20) plt.tight_layout() plt.show() . . 1. Tjur, T. (2009). Coefficients of Determination in Logistic Regression Models - A New Proposal: The Coefficient of Discrimination. The American Statistician, 63(4), 366-372. DOI: 10.1198/tast.2009.08210↩ . 2. For a review of sparsity indices, see Harley and Rickard (2009). Comparing Measures of Sparsity. arXiv. DOI: arXiv:0811.4706v2↩ .",
            "url": "https://banskt.github.io/glm-ash-notes/jupyter/2020/07/07/quasi-Laplace-mrash-logistic.html",
            "relUrl": "/jupyter/2020/07/07/quasi-Laplace-mrash-logistic.html",
            "date": " • Jul 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Comparing Quasi-Laplace approximation with true posterior",
            "content": "About . In the quasi-Laplace approximation, we apply the Laplace approximation to a regularized likelihood $ mathscr{L}_{ mathrm{reg}}( boldsymbol{ beta})$ defined as the product of the likelihood and a Gaussian regularizer, begin{equation*} mathscr{L}_{ mathrm{reg}}( boldsymbol{ beta}) triangleq p left( mathbf{y} mid mathbf{X}, boldsymbol{ beta} right) mathcal{N} left( boldsymbol{ beta} mid mathbf{0}, mathbf{ Lambda}^{-1} right) propto mathcal{N} left( boldsymbol{ beta} mid mathbf{m}, mathbf{S} right) end{equation*} such that the mode of the regularized likelihood is near the mode of the posterior. . Likelihood: $p left( mathbf{y} mid mathbf{X}, boldsymbol{ beta} right)$ | Regularized likelihood: $p left( mathbf{y} mid mathbf{X}, boldsymbol{ beta} right) mathcal{N} left( boldsymbol{ beta} mid mathbf{0}, mathbf{ Lambda}^{-1} right)$ | Prior: $p left( boldsymbol{ beta} mid g right)$ | Posterior: $p left( mathbf{y} mid mathbf{X}, boldsymbol{ beta} right) p left( boldsymbol{ beta} mid g right)$ | Marginal likelihood: $p left( mathbf{y} mid mathbf{X} right) = int p left( mathbf{y} mid mathbf{X}, boldsymbol{ beta} right) p left( boldsymbol{ beta} mid g right) d boldsymbol{ beta}$ | . Here, $g$ is the mixture of Gaussians with known variances but unknown mixture coefficients. The precision matrix $ mathbf{ Lambda}$ is defined as a diagonal matrix, $ mathbf{ Lambda} triangleq mathrm{diag} left( boldsymbol{ lambda} right)$, whose elements $ lambda_j$ are roughly set to some expected value to ensure that the regularized likelihood is centered at the mode of the posterior. . The quasi-Laplace approximation is helpful in cases where the marginal likelihood (and therefore, the posterior) is not analytically tractable, for example in logistic regression. . #collapse_hide import numpy as np np.set_printoptions(precision = 4, suppress=True) from scipy import optimize import matplotlib.pyplot as plt from matplotlib import cm from matplotlib import ticker as plticker from mpl_toolkits.axes_grid1 import make_axes_locatable import sys sys.path.append(&quot;../../utils/&quot;) import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;kelly&#39;, dpi = 72) . . Generate toy data . Let us consider a logistic model with sparse coefficients, so that the number of causal variables is much less than the number of variables nvar in the model. This is ensured by sampling the betas from a Gaussian mixture prior with variances given by $ sigma_k^2$ (sigmak2), where the numbers of components is given by nGcomp and the mixture coefficients given by probk, the sparsity of which is controlled by the variable sparsity that specifies the mixture coefficient of the component $ mathcal{N}(0, 0)$ (or the delta function). . nsample = 20 nvar = 30 nGcomp = 3 sparsity = 0.8 prior_strength = 5 num_inf = 1e4 # a large number for 1/sigma_k^2 when sigma_k^2 = 0 probk = np.zeros(nGcomp) probk[0] = sparsity probk[1:(nGcomp - 1)] = (1 - sparsity) / (nGcomp - 1) probk[nGcomp - 1] = 1 - np.sum(probk) sigmak2 = np.array([prior_strength * np.square(np.power(2, (i)/nGcomp) - 1) for i in range(nGcomp)]) . The data is generated from Bernoulli trials (equivalent to binomial distribution with $n=1$ and $p= sigma( mathbf{X} boldsymbol{ beta})$, where $ sigma( cdot)$ is the logistic function. $ mathbf{X}$ is centered and scaled such that for each variable $j$, the variance is $ mathrm{var}( mathbf{x}_j) = 1$. . # collapse-hide def standardize(X): Xnorm = (X - np.mean(X, axis = 0)) Xstd = Xnorm / np.std(Xnorm, axis = 0) return Xstd def logistic_data(X, beta): Xbeta = np.dot(X, beta) pred = 1 / (1 + np.exp(-Xbeta)) Y = np.random.binomial(1, pred) return Y X = np.random.rand(nsample * nvar).reshape(nsample, nvar) X = standardize(X) gammajk = np.random.multinomial(1, probk, size = nvar) beta = np.zeros(nvar) for j in range(nvar): if gammajk[j, 0] != 1: kidx = np.where(gammajk[j, :] == 1)[0][0] kstd = np.sqrt(sigmak2[kidx]) beta[j] = np.random.normal(loc = 0., scale = kstd) ncausal = beta[beta != 0].shape[0] betavar = np.var(beta[beta != 0]) Y = logistic_data(X, beta) . . Let us have a look at the generated data. . # collapse-hide print(f&#39;There are {ncausal} non-zero coefficients with variance {betavar:.4f}&#39;) fig = plt.figure(figsize = (6,6)) ax1 = fig.add_subplot(111) Xbeta = np.dot(X, beta) pred = 1 / (1 + np.exp(-Xbeta)) ax1.scatter(Xbeta, Y, s = 10) ax1.scatter(Xbeta, pred, s = 10) ax1.set_xlabel(r&#39;$ sum_i X_{ni} beta_i$&#39;) ax1.set_ylabel(r&#39;$Y_n$&#39;) plt.show() . . There are 5 non-zero coefficients with variance 0.5920 . True posterior vs quasi-Laplace posterior . We select two causal variables (with maximum effect size) and fix all the others to optimum values to understand how the likelihood and posterior depends on these two chosen variables. To avoid the sum over the indicator variables, we use the joint prior $p left( boldsymbol{ beta}, boldsymbol{ gamma} mid g right)$. . Some useful function definitions: . # collapse-hide def get_log_likelihood(Y, X, beta): Xbeta = np.dot(X, beta) logL = np.sum(Y * Xbeta - np.log(1 + np.exp(Xbeta))) return logL def get_log_prior(beta, gammajk, probk, sigmak2): logprior = 0 for j, b in enumerate(beta): k = np.where(gammajk[j, :] == 1)[0][0] logprior += np.log(probk[k]) if k &gt; 0: logprior += - 0.5 * (np.log(2 * np.pi) + np.log(sigmak2[k]) + b * b / sigmak2[k]) return logprior def plot_contours(ax, X, Y, Z, beta, norm, cstep = 10, zlabel = &quot;&quot;): zmin = np.min(Z) - 1 * np.std(Z) zmax = np.max(Z) + 1 * np.std(Z) ind = np.unravel_index(np.argmax(Z, axis=None), Z.shape) levels = np.linspace(zmin, zmax, 200) clevels = np.linspace(zmin, zmax, 20) cmap = cm.YlOrRd_r if norm: cset1 = ax.contourf(X, Y, Z, levels, norm = norm, cmap=cm.get_cmap(cmap, len(levels) - 1)) else: cset1 = ax.contourf(X, Y, Z, levels, cmap=cm.get_cmap(cmap, len(levels) - 1)) cset2 = ax.contour(X, Y, Z, clevels, colors=&#39;k&#39;) for c in cset2.collections: c.set_linestyle(&#39;solid&#39;) ax.set_aspect(&quot;equal&quot;) ax.scatter(beta[0], beta[1], color = &#39;blue&#39;, s = 100) ax.scatter(X[ind[1]], Y[ind[0]], color = &#39;k&#39;, s = 100) divider = make_axes_locatable(ax) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.2) cbar = plt.colorbar(cset1, cax=cax) ytickpos = np.arange(int(zmin / cstep) * cstep, zmax, cstep) cbar.set_ticks(ytickpos) if zlabel: cax.set_ylabel(zlabel) #loc = plticker.AutoLocator() #ax.xaxis.set_major_locator(loc) #ax.yaxis.set_major_locator(loc) def log_likelihood(beta, X, Y, L): nvar = beta.shape[0] Xbeta = np.dot(X, beta) ## Function llb = np.sum(np.dot(Y, Xbeta) - np.log(1 + np.exp(Xbeta))) #reg = 0.5 * np.sum(np.log(L)) - 0.5 * np.einsum(&#39;i,i-&gt;i&#39;, np.square(beta), L) reg = - 0.5 * np.einsum(&#39;i,i-&gt;i&#39;, np.square(beta), L) loglik = llb + reg ## Gradient pred = 1 / (1 + np.exp(-Xbeta)) der = np.einsum(&#39;i,ij-&gt;j&#39;, (Y - pred), X) - np.multiply(beta, L) return -loglik, -der def precisionLL(X, beta, L): nvar = X.shape[1] Xbeta = np.einsum(&#39;i,ji-&gt;j&#39;, beta, X) pred = 1 / (1 + np.exp(-Xbeta)) hess = - np.einsum(&#39;i,i,ij,ik-&gt;jk&#39;, pred, (1 - pred), X, X) hess[np.diag_indices(nvar)] -= L return -hess def get_mS(X, Y, beta0, L): nvar = X.shape[1] args = X, Y, L gmode = optimize.minimize(log_likelihood, beta0, args=args, method=&#39;L-BFGS-B&#39;, jac=True, bounds=None, options={&#39;maxiter&#39;: 20000000, &#39;maxfun&#39;: 20000000, &#39;ftol&#39;: 1e-9, &#39;gtol&#39;: 1e-9 #&#39;disp&#39;: True }) M = gmode.x Sinv = precisionLL(X, M, L) return M, Sinv def get_qL_log_posterior(beta, L, M, Sinv, logdetSinv, logprior): blessM = beta - M bMSbM = np.dot(blessM.T, np.dot(Sinv, blessM)) bLb = np.einsum(&#39;i, i&#39;, np.square(beta), L) logdetLinv = - np.sum(np.log(L)) logposterior = 0.5 * (logdetSinv + logdetLinv - bMSbM + bLb) logposterior += logprior return logposterior . . We calculate the likelihood, prior, true posterior and the quasi-Laplace posterior. Note that the posteriors are not normalized. We apply quasi-Laplace (QL) approximation with some $ mathbf{ Lambda}$ and show QL posterior distribution, which is given by begin{equation*} p left( mathbf{y} mid mathbf{X}, boldsymbol{ beta} right) p left( boldsymbol{ beta} mid g right) propto frac{ mathcal{N} left( boldsymbol{ beta} mid mathbf{m}, mathbf{S} right) p left( boldsymbol{ beta} mid g right) }{ mathcal{N} left( boldsymbol{ beta} mid mathbf{0}, mathbf{ Lambda}^{-1} right) } end{equation*} Here, we assume that we know $ mathbf{ Lambda}$. In reality, we will not know $ mathbf{ Lambda}$ but will have to learn it from the data or make some educated guess from the prior choice. . # collapse-hide bchoose = np.argsort(abs(beta))[-2:] nplotx = 20 nploty = 20 b1min = -5 b1max = 5 b2min = -5 b2max = 5 beta1 = np.linspace(b1min, b1max, nplotx) beta2 = np.linspace(b2min, b2max, nploty) logL = np.zeros((nploty, nplotx)) logPr = np.zeros((nploty, nplotx)) logPs = np.zeros((nploty, nplotx)) logQL = np.zeros((nploty, nplotx)) thisbeta = beta.copy() mask = np.ones(nvar, bool) mask[bchoose] = False true_pi = np.sum(gammajk, axis = 0) / np.sum(gammajk) #reg = 1 / np.einsum(&#39;i,i&#39;, true_pi, sigmak2) #regL = np.repeat(reg, nvar) regL = np.repeat(num_inf, nvar) for j, b in enumerate(beta): k = np.where(gammajk[j, :] == 1)[0][0] if k &gt; 0: regL[j] = 1 / sigmak2[k] M, Sinv = get_mS(X, Y, beta, regL) sgndetSinv, logdetSinv = np.linalg.slogdet(Sinv) for i, b1 in enumerate(beta1): for j, b2 in enumerate(beta2): thisbeta[bchoose] = np.array([b1, b2]) logL[j, i] = get_log_likelihood(Y, X, thisbeta) logPr[j, i] = get_log_prior(thisbeta, gammajk, probk, sigmak2) logQL[j, i] = get_qL_log_posterior(thisbeta, regL, M, Sinv, logdetSinv, logPr[j, i]) logPs = logL + logPr . . Here, we plot the contour maps. The x and y-axis show the two coefficients $ beta_1$ and $ beta_2$, which we chose to vary. The blue dot shows the coordinates of the true values of $ { beta_1, beta_2 }$ and the black dot shows the maximum of the log probabilities. Note how the non-Gaussian true posterior is now approximated by a Gaussian QL posterior. . fig = plt.figure(figsize = (12, 8)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) norm = cm.colors.Normalize(vmin=np.min(logPs), vmax=np.max(logPs)) plot_contours(ax1, beta1, beta2, logL, beta[bchoose], None, cstep = 50, zlabel = &quot;Log Likelihood&quot;) plot_contours(ax2, beta1, beta2, logPr, beta[bchoose], None, cstep = 20, zlabel = &quot;Log Prior&quot;) plot_contours(ax3, beta1, beta2, logPs, beta[bchoose], None, cstep = 50, zlabel = &quot;Log Posterior&quot;) plot_contours(ax4, beta1, beta2, logQL, beta[bchoose], None, cstep = 40, zlabel = &quot;Log QL Posterior&quot;) plt.tight_layout() plt.show() . Effect of regularizer . Let us assume that we do not know $ mathbf{ Lambda}$ and all $ lambda_j$&#39;s are equal. Here we look at how the QL posterior changes with varying $ beta_2$ for different values of $ lambda_j$. . Here I define a function for calculating the QL posterior and true posterior for changing the coefficients of a single variable: . # collapse-hide def get_logQL_logPs_single_variable(X, Y, beta, regvar, betavar, bidx, regL, gammajk, probk, sigmak2): nvar = beta.shape[0] nplotrv = regvar.shape[0] nplotx = betavar.shape[0] logL = np.zeros(nplotx) logPr = np.zeros(nplotx) logPs = np.zeros(nplotx) logQL = np.zeros((nplotrv, nplotx)) thisbeta = beta.copy() thisL = regL.copy() for j, b2 in enumerate(betavar): thisbeta[bidx] = b2 logL[j] = get_log_likelihood(Y, X, thisbeta) logPr[j] = get_log_prior(thisbeta, gammajk, probk, sigmak2) logPs = logL + logPr for i, r1 in enumerate(regvar): thisL = np.repeat(r1, nvar) #thisL[bidx] = r1 M, Sinv = get_mS(X, Y, beta, thisL) sgndetSinv, logdetSinv = np.linalg.slogdet(Sinv) for j, b2 in enumerate(betavar): thisbeta[bidx] = b2 logQL[i, j] = get_qL_log_posterior(thisbeta, thisL, M, Sinv, logdetSinv, logPr[j]) return logPs, logQL . . And then look at the posteriors for $ beta_2$. . #collapse-hide nplotx_rv = 200 nplotrv = 4 bmin = -10 bmax = 15 rvmin = 1 rvmax = 10 bidx = bchoose[1] fig = plt.figure(figsize = (8, 8)) ax1 = fig.add_subplot(111) betavals = np.linspace(bmin, bmax, nplotx_rv) regvals = np.linspace(rvmin, rvmax, nplotrv) logPs_rv, logQL_rv = get_logQL_logPs_single_variable(X, Y, beta, regvals, betavals, bidx, regL, gammajk, probk, sigmak2) ax1.plot(betavals, logPs_rv, lw = 3, zorder = 2, label = &#39;True Posterior&#39;) ax1.scatter(betavals[np.argmax(logPs_rv)], logPs_rv[np.argmax(logPs_rv)], s = 40, zorder = 10, color = &#39;black&#39;) for i, r1 in enumerate(regvals): ax1.plot(betavals, logQL_rv[i, :], lw = 2, zorder = 5, label = f&#39;$ lambda =${r1:.2f}&#39;) ix = np.argmax(logQL_rv[i, :]) ax1.scatter(betavals[ix], logQL_rv[i, ix], s = 40, zorder = 10, color = &#39;black&#39;) ax1.axvline(beta[bidx], ls = &#39;dotted&#39;, zorder = 1) ax1.legend(handlelength = 1.5) ax1.set_xlabel(r&#39;$ beta$&#39;) ax1.set_ylabel(&#39;Log Posterior&#39;) plt.tight_layout() plt.show() . . What happens to the QL posterior for other variables? Let us look at every $ beta_j$ and their individual maximum posterior, while all others are kept fixed at optimum values. Here, I have arranged the $ beta_j$ in ascending order for easy viewing. . #collapse-hide bidx_sorted = np.argsort(beta) bidx_sorted_nz = bidx_sorted #bidx_sorted[beta[bidx_sorted]!=0] maxQL = np.zeros((nplotrv, len(bidx_sorted_nz))) maxPs = np.zeros(len(bidx_sorted_nz)) for i, bidx in enumerate(bidx_sorted_nz): _logPs, _logQL = get_logQL_logPs_single_variable(X, Y, beta, regvals, betavals, bidx, regL, gammajk, probk, sigmak2) maxPs[i] = betavals[np.argmax(_logPs)] for j in range(len(regvals)): maxQL[j, i] = betavals[np.argmax(_logQL[j, :])] fig = plt.figure(figsize = (16, 8)) ax1 = fig.add_subplot(111) xvals = np.arange(len(bidx_sorted_nz)) ax1.plot(xvals, maxPs, lw = 2, zorder = 2, label = &#39;True Posterior&#39;) for i, r1 in enumerate(regvals): ax1.plot(xvals, maxQL[i, :], lw = 2, zorder = 5, label = f&#39;$ lambda =${r1:.2f}&#39;) ax1.scatter(xvals, maxPs, s = 20, zorder = 1) for i, r1 in enumerate(regvals): ax1.scatter(xvals, maxQL[i, :], s = 20, zorder = 1) ax1.scatter(xvals, beta[bidx_sorted_nz], s = 80, zorder = 10, color = &#39;maroon&#39;, label = &#39;Input&#39;) ax1.legend(handlelength = 1.5) ax1.set_xlabel(r&#39;Index of $ beta$&#39;) ax1.set_ylabel(r&#39;$ beta$ at maximum log posterior&#39;) ax1.set_xticks(xvals) ax1.set_xticklabels([f&#39;{idx}&#39; for idx in bidx_sorted_nz]) plt.tight_layout() plt.show() . .",
            "url": "https://banskt.github.io/glm-ash-notes/jupyter/2020/06/22/intuition-for-quasi-Laplace.html",
            "relUrl": "/jupyter/2020/06/22/intuition-for-quasi-Laplace.html",
            "date": " • Jun 22, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://banskt.github.io/glm-ash-notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://banskt.github.io/glm-ash-notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}